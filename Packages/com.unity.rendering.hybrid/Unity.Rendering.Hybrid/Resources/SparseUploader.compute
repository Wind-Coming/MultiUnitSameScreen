#pragma kernel CopyKernel

static const uint kOperationType_Upload = 0;
static const uint kOperationType_Matrix = 1;
static const uint kOperationType_Matrix_Inverse = 2;

struct Operation
{
    uint type;
    uint srcOffset;
    uint dstOffset;
    uint dstOffsetExtra;
    uint size;
    uint count;
};

StructuredBuffer<Operation> operations;

ByteAddressBuffer src;
RWByteAddressBuffer dst;

#define NUM_THREADS 64
#define NUM_BYTES_PER_THREAD 4
#define NUM_BYTES_PER_GROUP (NUM_THREADS * NUM_BYTES_PER_THREAD)

void CopyToGPU(Operation operation, uint threadID)
{
    uint numFullWaves = (operation.size * operation.count) / NUM_BYTES_PER_GROUP;

    uint srcIndex = (threadID * NUM_BYTES_PER_THREAD) % operation.size;
    uint dstIndex = threadID * NUM_BYTES_PER_THREAD;
    for (uint i = 0; i < numFullWaves; i += 1)
    {
        uint val = src.Load(srcIndex + operation.srcOffset);
		dst.Store(dstIndex + operation.dstOffset, val);

        srcIndex = (srcIndex + NUM_BYTES_PER_GROUP) % operation.size;
        dstIndex += NUM_BYTES_PER_GROUP;
    }

    if (dstIndex < (operation.size * operation.count))
    {
        uint val = src.Load(srcIndex + operation.srcOffset);
		dst.Store(dstIndex + operation.dstOffset, val);
    }
}

float4x4 loadMatrixSrc(ByteAddressBuffer buf, uint offset)
{
    float4 p1 = asfloat(buf.Load4(offset +  0));
    float4 p2 = asfloat(buf.Load4(offset + 16));
    float4 p3 = asfloat(buf.Load4(offset + 32));

    return float4x4(
        p1.x, p1.y, p1.z, 0.0,
        p1.w, p2.x, p2.y, 0.0,
        p2.z, p2.w, p3.x, 0.0,
        p3.y, p3.z, p3.w, 1.0
    );
}

void storeMatrixDst(RWByteAddressBuffer buf, uint offset, float4x4 mat)
{
    buf.Store4(offset +  0, asuint(mat[0]));
    buf.Store4(offset + 16, asuint(mat[1]));
    buf.Store4(offset + 32, asuint(mat[2]));
    buf.Store4(offset + 48, asuint(mat[3]));
}

float csum(float3 v)
{
    return v.x + v.y + v.z;
}

float3x3 inverse3x3(float3x3 m)
{
    float3 c0 = m._m00_m10_m20;
    float3 c1 = m._m01_m11_m21;
    float3 c2 = m._m02_m12_m22;

    float3 t0 = float3(c1.x, c2.x, c0.x);
    float3 t1 = float3(c1.y, c2.y, c0.y);
    float3 t2 = float3(c1.z, c2.z, c0.z);

    float3 m0 = t1 * t2.yzx - t1.yzx * t2;
    float3 m1 = t0.yzx * t2 - t0 * t2.yzx;
    float3 m2 = t0 * t1.yzx - t0.yzx * t1;

    float rcpDet = 1.0f / csum(t0.zxy * m0);
    return float3x3(m0, m1, m2) * rcpDet;
}

float4x4 inverseAffine(float4x4 m)
{
    float3x3 inv = inverse3x3((float3x3)m);
    float3 trans = float3(m._m30, m._m31, m._m32);
    float3 invTrans = -mul(trans, inv);
    return float4x4(
        inv._m00,   inv._m10,   inv._m20,   0.0,
        inv._m01,   inv._m11,   inv._m21,   0.0,
        inv._m02,   inv._m12,   inv._m22,   0.0,
        invTrans.x, invTrans.y, invTrans.z, 1.0);
}

void CopyMatrix(Operation operation, uint din, uint dout)
{
    float4x4 orig = loadMatrixSrc(src, operation.srcOffset + din);
    storeMatrixDst(dst, operation.dstOffset + dout, orig);
}

void CopyAndInverseMatrix(Operation operation, uint din, uint dout)
{
    float4x4 orig = loadMatrixSrc(src, operation.srcOffset + din);
    float4x4 inv = inverseAffine(orig);
    storeMatrixDst(dst, operation.dstOffset + dout, orig);
    storeMatrixDst(dst, operation.dstOffsetExtra + dout, inv);
}

#define NUM_MATRIX_INPUT_BYTES_PER_THREAD 48
#define NUM_MATRIX_INPUT_BYTES_PER_GROUP (NUM_THREADS * NUM_MATRIX_INPUT_BYTES_PER_THREAD)
#define NUM_MATRIX_OUTPUT_BYTES_PER_THREAD 64
#define NUM_MATRIX_OUTPUT_BYTES_PER_GROUP (NUM_THREADS * NUM_MATRIX_OUTPUT_BYTES_PER_THREAD)

void MatricesToGPU(Operation operation, uint threadID)
{
    uint numFullWaves = operation.size / NUM_MATRIX_INPUT_BYTES_PER_GROUP;
    uint inputIndex = threadID * NUM_MATRIX_INPUT_BYTES_PER_THREAD;
    uint outputIndex = threadID * NUM_MATRIX_OUTPUT_BYTES_PER_THREAD;

    for (uint i = 0; i < numFullWaves; ++i)
    {
        CopyMatrix(operation, inputIndex, outputIndex);
        inputIndex += NUM_MATRIX_INPUT_BYTES_PER_GROUP;
        outputIndex += NUM_MATRIX_OUTPUT_BYTES_PER_GROUP;
    }

    if (inputIndex < operation.size)
    {
        CopyMatrix(operation, inputIndex, outputIndex);
    }
}

void InverseMatricesToGPU(Operation operation, uint threadID)
{
    uint numFullWaves = operation.size / NUM_MATRIX_INPUT_BYTES_PER_GROUP;
    uint inputIndex = threadID * NUM_MATRIX_INPUT_BYTES_PER_THREAD;
    uint outputIndex = threadID * NUM_MATRIX_OUTPUT_BYTES_PER_THREAD;

    for (uint i = 0; i < numFullWaves; ++i)
    {
        CopyAndInverseMatrix(operation, inputIndex, outputIndex);
        inputIndex += NUM_MATRIX_INPUT_BYTES_PER_GROUP;
        outputIndex += NUM_MATRIX_OUTPUT_BYTES_PER_GROUP;
    }

    if (inputIndex < operation.size)
    {
        CopyAndInverseMatrix(operation, inputIndex, outputIndex);
    }
}

[numthreads(NUM_THREADS, 1, 1)]
void CopyKernel(uint threadID : SV_GroupThreadID, uint groupID : SV_GroupID)
{
    Operation operation = operations[groupID];

    if (operation.type == kOperationType_Upload)
        CopyToGPU(operation, threadID);
    else if (operation.type == kOperationType_Matrix)
        MatricesToGPU(operation, threadID);
    else if (operation.type == kOperationType_Matrix_Inverse)
        InverseMatricesToGPU(operation, threadID);
}
